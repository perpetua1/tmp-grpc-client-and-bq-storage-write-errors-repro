"""
This file demonstrates the error log being generated by the bidi.BackgroundConsumer on shutdown.

It uses no dependencies
"""

import argparse
import logging
import random

from google.cloud import bigquery
from google.cloud.bigquery_storage_v1.types import ProtoRows, ProtoSchema
from google.cloud.bigquery_storage_v1.types.storage import AppendRowsRequest
from google.cloud.bigquery_storage_v1.writer import AppendRowsStream
from google.protobuf import descriptor_pb2
from google.cloud.bigquery_storage_v1 import BigQueryWriteClient

from my_service_pb2 import PingResponse

logger = logging.getLogger()


def get_create_table_sql(args):
    return f"""
    CREATE TABLE IF NOT EXISTS `{args.project_id}.{args.dataset_id}.{args.table_id}` (
        number INT64,
        current_ts_ms INT64
    )
    OPTIONS(
        -- Any test dataset should already have a default expiration time, but explicitly set one as a 
        -- matter of good practice.
        expiration_timestamp = TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 12 HOUR)
    )
    """


def do_insert_via_append_rows_stream(args):
    bq_storage_client = BigQueryWriteClient()
    descriptor = descriptor_pb2.DescriptorProto()
    PingResponse.DESCRIPTOR.CopyToProto(descriptor)

    #
    grpc_stream = AppendRowsStream(
        bq_storage_client,
        initial_request_template=AppendRowsRequest(
            write_stream=f"{bq_storage_client.table_path(args.project_id, args.dataset_id, args.table_id)}/streams/_default",
            proto_rows=AppendRowsRequest.ProtoData(
                writer_schema=ProtoSchema(proto_descriptor=descriptor),
            ),
        ),
    )

    f = grpc_stream.send(
        AppendRowsRequest(
            proto_rows=AppendRowsRequest.ProtoData(
                rows=ProtoRows(
                    serialized_rows=[
                        PingResponse(
                            number=random.randint(1, 2**63)
                        ).SerializeToString()
                    ]
                )
            )
        )
    )
    f.result()

    grpc_stream.close()

    logger.debug("Stream closed, insertion success")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Repro for grpc error when using the BQ storage write API"
    )

    parser.add_argument(
        "--project-id", type=str, required=True, help="Google cloud project id to use"
    )
    parser.add_argument(
        "--dataset-id", type=str, required=True, help="BQ dataset to use"
    )
    parser.add_argument(
        "--table-id", type=str, default="test_table", help="BQ table to use"
    )

    parser.add_argument(
        "--create-table",
        action="store_true",
        help="Run the SQL to create the testing table",
    )
    parser.add_argument(
        "--show-create-table-sql",
        action="store_true",
        help="Print the SQL to create the testing table. Use this if you want to run manually",
    )

    args = parser.parse_args()

    # Setup logging
    logging.basicConfig(
        level=logging.DEBUG,
        format="%(asctime)s - %(levelname)s - %(name)s - %(threadName)s - %(message)s",
        handlers=[logging.StreamHandler()],
    )

    create_table_sql = get_create_table_sql(args)
    if args.show_create_table_sql:
        print()
        print("Create table SQL:")
        print(create_table_sql)
        print()

    if args.create_table:
        bq_client = bigquery.Client(project=args.project_id)
        bq_client.query(create_table_sql).result()

    do_insert(args)